{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d974e0-b1f4-4bc5-804e-416f2a3e60e9",
   "metadata": {},
   "source": [
    "# PixelCNN\n",
    "\n",
    "As explained at the end of the Na√Øve Face Generation notebook, to effectively generate images we need to sample each pixels given te surrounding one. This notebook presents an early approach to construct a model able to do that called **PixelCNN**. \n",
    "\n",
    "To formalize mathematically the problem, $p(x)$ represent the pixels distribution of an images. Which is equivalent to the joint probability of each pixels $p(x_1, x_2, ..., x_n)$. Modelizing the whole joint probability is too expensive, thus we made the hypothesis that $p(x_i) = p(x_i | x_{i-1})p(x_{i-1})$. Meaning that te probability of the pixel $x_i$ only depends on the previous pixel.\n",
    "\n",
    "This simplification makes it easier to compute the whole probability of an images that could be rewritte like that given our assumption :\n",
    "$p(x_n | x_{n-1})...p(x_2 | x_1)p(x_1)$\n",
    "\n",
    "PixelCNN will learn the parameter of those distribution based on example. In this notebook we are going to generate numbers from the **mnist** dataset. First part of the code will focus on the loading and pre-processing of the dataset.\n",
    "\n",
    "The dataset chose is the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. It consists of 28x28 images of handwritten digits. We preprocess the image to make them binary. Meaning that the background of the image have a value of 0 and the foreground of the image have a value of 1 without gray scale. This preprocessing will help the model to generate better numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74dec488-b374-43fd-9534-452df48f33f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2448x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.mnist import *\n",
    "\n",
    "mnist_path = \"./ressources/mnist\"\n",
    "\n",
    "dataset = load_mnist(mnist_path, binary_images=True)\n",
    "\n",
    "plot_images(dataset[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f45f5",
   "metadata": {},
   "source": [
    "## Masked Convolution\n",
    "\n",
    "Our first Generator Network leverage the successful Convolutional Neural Network architecture. However, classic CNN are able to look at a pixel and all the surrounding pixels. As we are trying to generate the next pixel based on the previous one, the Convolutionnal Layer should be constrained to only look at the previous pixel. \n",
    "\n",
    "The next cell shows the implementation of such a CNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9185e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code by Hrituraj Singh\n",
    "Indian Institute of Technology Roorkee\n",
    "'''\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "class MaskedCNN(nn.Conv2d):\n",
    "\t\"\"\"\n",
    "\tImplementation of Masked CNN Class as explained in A Oord et. al. \n",
    "\tTaken from https://github.com/jzbontar/pixelcnn-pytorch\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, mask_type, *args, **kwargs):\n",
    "\t\tself.mask_type = mask_type\n",
    "\t\tassert mask_type in ['A', 'B'], \"Unknown Mask Type\"\n",
    "\t\tsuper(MaskedCNN, self).__init__(*args, **kwargs)\n",
    "\t\tself.register_buffer('mask', self.weight.data.clone())\n",
    "\n",
    "\t\t_, depth, height, width = self.weight.size()\n",
    "\t\tself.mask.fill_(1)\n",
    "\t\tif mask_type =='A':\n",
    "\t\t\tself.mask[:,:,height//2,width//2:] = 0\n",
    "\t\t\tself.mask[:,:,height//2+1:,:] = 0\n",
    "\t\telse:\n",
    "\t\t\tself.mask[:,:,height//2,width//2+1:] = 0\n",
    "\t\t\tself.mask[:,:,height//2+1:,:] = 0\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tself.weight.data*=self.mask\n",
    "\t\treturn super(MaskedCNN, self).forward(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c3f8d",
   "metadata": {},
   "source": [
    "## PixelCNN\n",
    "\n",
    "Now that we have our special MaskedCNN layer, we can use it in the architecture of our PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f6b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicated from https://github.com/singh-hrituraj/PixelCNN-Pytorch/blob/master/Model.py\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "\t\"\"\"\n",
    "\tNetwork of PixelCNN as described in A Oord et. al. \n",
    "\t\"\"\"\n",
    "\tdef __init__(self, no_layers=8, kernel = 7, channels=64, device=None):\n",
    "\t\tsuper(PixelCNN, self).__init__()\n",
    "\t\tself.no_layers = no_layers\n",
    "\t\tself.kernel = kernel\n",
    "\t\tself.channels = channels\n",
    "\t\tself.layers = {}\n",
    "\t\tself.device = device\n",
    "\n",
    "\t\tself.Conv2d_1 = MaskedCNN('A',1,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_1 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_1= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_2 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_2 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_2= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_3 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_3 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_3= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_4 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_4 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_4= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_5 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_5 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_5= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_6 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_6 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_6= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_7 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_7 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_7= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_8 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_8 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_8= nn.ReLU(True)\n",
    "\n",
    "\t\tself.out = nn.Conv2d(channels, 256, 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.Conv2d_1(x)\n",
    "\t\tx = self.BatchNorm2d_1(x)\n",
    "\t\tx = self.ReLU_1(x)\n",
    "\n",
    "\t\tx = self.Conv2d_2(x)\n",
    "\t\tx = self.BatchNorm2d_2(x)\n",
    "\t\tx = self.ReLU_2(x)\n",
    "\n",
    "\t\tx = self.Conv2d_3(x)\n",
    "\t\tx = self.BatchNorm2d_3(x)\n",
    "\t\tx = self.ReLU_3(x)\n",
    "\n",
    "\t\tx = self.Conv2d_4(x)\n",
    "\t\tx = self.BatchNorm2d_4(x)\n",
    "\t\tx = self.ReLU_4(x)\n",
    "\n",
    "\t\tx = self.Conv2d_5(x)\n",
    "\t\tx = self.BatchNorm2d_5(x)\n",
    "\t\tx = self.ReLU_5(x)\n",
    "\n",
    "\t\tx = self.Conv2d_6(x)\n",
    "\t\tx = self.BatchNorm2d_6(x)\n",
    "\t\tx = self.ReLU_6(x)\n",
    "\n",
    "\t\tx = self.Conv2d_7(x)\n",
    "\t\tx = self.BatchNorm2d_7(x)\n",
    "\t\tx = self.ReLU_7(x)\n",
    "\n",
    "\t\tx = self.Conv2d_8(x)\n",
    "\t\tx = self.BatchNorm2d_8(x)\n",
    "\t\tx = self.ReLU_8(x)\n",
    "\n",
    "\t\treturn self.out(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091845e",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n",
    "\n",
    "Now that we have our PixelCNN architecture, we have to train it in order to generate images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2ea563",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [1, 1, 7, 7], but got 3-dimensional input of size [64, 28, 28] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-79b0581b9e7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-85a7eacb3819>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e820cb41c07e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m*=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaskedCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [1, 1, 7, 7], but got 3-dimensional input of size [64, 28, 28] instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "batch_size = 64\n",
    "epoch = 5\n",
    "\n",
    "\n",
    "train = data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = PixelCNN(channels=1).to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(epoch):\n",
    "  for images, labels in train:\n",
    "\t\t\t\n",
    "\t\t\ttarget = Variable(images[:,0,:]*255).long()\n",
    "\t\t\timages = images.to(device)\n",
    "\t\t\ttarget = target.to(device)\n",
    "\t\t\t\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\toutput = net(images)\n",
    "\t\t\tloss = criterion(output, target)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python (3.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
